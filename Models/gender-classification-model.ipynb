{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ***So Lets start***","metadata":{"id":"tnUfeoQ7U0Z0"}},{"cell_type":"code","source":"# Make sure to install torch and torchvisionif running locally on jupyter notebook","metadata":{"id":"dYbAavWrzWME","execution":{"iopub.status.busy":"2022-03-13T06:13:32.827427Z","iopub.execute_input":"2022-03-13T06:13:32.827771Z","iopub.status.idle":"2022-03-13T06:13:32.831732Z","shell.execute_reply.started":"2022-03-13T06:13:32.827727Z","shell.execute_reply":"2022-03-13T06:13:32.831009Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"*Importing important modules*","metadata":{"id":"Agkzy_iBWkkR"}},{"cell_type":"code","source":"!pip install opendatasets \nimport os\nimport torch\nimport torchvision\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nimport opendatasets as od","metadata":{"id":"XU4CjcSR1R3F","outputId":"af98aa6e-ab90-4288-c2c7-10caf142db27","execution":{"iopub.status.busy":"2022-03-13T06:13:32.833595Z","iopub.execute_input":"2022-03-13T06:13:32.834244Z","iopub.status.idle":"2022-03-13T06:13:41.978119Z","shell.execute_reply.started":"2022-03-13T06:13:32.834206Z","shell.execute_reply":"2022-03-13T06:13:41.977338Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Downloading and preparing data by applying transforms on PIL images to convert them to required tensors.**","metadata":{"id":"vgFjIWfwWrS3"}},{"cell_type":"code","source":"# uncomment if running on platform other than kaggle then would be required to enter kaggle credentials\n# also then remove '/kaggle/input/' from data_dir\n# # Dowload the dataset\n# od.download('https://www.kaggle.com/c/fake-image-classification-challenge')","metadata":{"id":"UYIe1yR21lTe","outputId":"1f75519b-9113-483f-c04e-62f00a75521b","execution":{"iopub.status.busy":"2022-03-13T06:13:41.981468Z","iopub.execute_input":"2022-03-13T06:13:41.981688Z","iopub.status.idle":"2022-03-13T06:13:41.987894Z","shell.execute_reply.started":"2022-03-13T06:13:41.981660Z","shell.execute_reply":"2022-03-13T06:13:41.987176Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-13T06:13:41.990902Z","iopub.execute_input":"2022-03-13T06:13:41.991109Z","iopub.status.idle":"2022-03-13T06:13:41.996006Z","shell.execute_reply.started":"2022-03-13T06:13:41.991085Z","shell.execute_reply":"2022-03-13T06:13:41.995271Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/gender-classification-original')","metadata":{"id":"uVQR1fIt10HZ","outputId":"ff8db067-185e-4f0e-d958-c0959f05ec6b","execution":{"iopub.status.busy":"2022-03-13T06:13:41.998649Z","iopub.execute_input":"2022-03-13T06:13:41.998932Z","iopub.status.idle":"2022-03-13T06:13:42.021704Z","shell.execute_reply.started":"2022-03-13T06:13:41.998897Z","shell.execute_reply":"2022-03-13T06:13:42.021064Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# uncomment this if running on any other platform\n# data_dir = 'fake-image-classification-challenge/data'\ndata_dir = '../input/gender-classification-original/Gender_Data_old'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir)\nprint(classes)","metadata":{"id":"uBHniTPj3PLu","outputId":"b651c60b-02eb-4ad6-c43f-59da765c6c4d","execution":{"iopub.status.busy":"2022-03-13T06:13:42.022710Z","iopub.execute_input":"2022-03-13T06:13:42.023004Z","iopub.status.idle":"2022-03-13T06:13:42.033074Z","shell.execute_reply.started":"2022-03-13T06:13:42.022969Z","shell.execute_reply":"2022-03-13T06:13:42.032445Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"female_face_files = os.listdir(data_dir + \"/Females\")\nprint('No. of training examples for female faces:', len(female_face_files))\nprint(female_face_files[:5])","metadata":{"id":"JZT98tKv3mfF","outputId":"298e0448-18ae-484f-a4f1-90e76f5d8a87","execution":{"iopub.status.busy":"2022-03-13T06:13:42.035010Z","iopub.execute_input":"2022-03-13T06:13:42.035724Z","iopub.status.idle":"2022-03-13T06:13:42.517540Z","shell.execute_reply.started":"2022-03-13T06:13:42.035690Z","shell.execute_reply":"2022-03-13T06:13:42.516829Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"male_face_files = os.listdir(data_dir + \"/Males\")\nprint('No. of training examples for male faces:', len(male_face_files))\nprint(male_face_files[:5])","metadata":{"id":"Tbj0ypLC4McN","outputId":"9e3b10fe-0c0a-4bc9-bd63-2364089934fd","execution":{"iopub.status.busy":"2022-03-13T06:13:42.518890Z","iopub.execute_input":"2022-03-13T06:13:42.519372Z","iopub.status.idle":"2022-03-13T06:13:43.078735Z","shell.execute_reply.started":"2022-03-13T06:13:42.519335Z","shell.execute_reply":"2022-03-13T06:13:43.077983Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors.","metadata":{"id":"H8mLLU3W4bxw"}},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\n\ndataset = ImageFolder(data_dir, tt.Compose([tt.Resize((96,96)), \n                                                     tt.RandomHorizontalFlip(), \n                                                      #tt.RandomResizedCrop(64, scale=(0.5,0.9), ratio=(1, 1)), \n                                            #tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                            #tt.RandomCrop(64, padding=4, padding_mode='reflect'),\n                                            tt.ToTensor(),\n                                            tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n                                            ]))","metadata":{"id":"Pc2HkNdC4fRD","execution":{"iopub.status.busy":"2022-03-13T06:13:43.080200Z","iopub.execute_input":"2022-03-13T06:13:43.080697Z","iopub.status.idle":"2022-03-13T06:14:02.712987Z","shell.execute_reply.started":"2022-03-13T06:13:43.080660Z","shell.execute_reply":"2022-03-13T06:14:02.712291Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"type(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T06:14:02.714184Z","iopub.execute_input":"2022-03-13T06:14:02.714613Z","iopub.status.idle":"2022-03-13T06:14:02.719999Z","shell.execute_reply.started":"2022-03-13T06:14:02.714576Z","shell.execute_reply":"2022-03-13T06:14:02.719376Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 142x142 px color images with 3 channels (RGB), each image tensor has the shape `(3,142,142)`.","metadata":{"id":"rrJ1mIzA4leP"}},{"cell_type":"code","source":"img, label = dataset[0]\nprint(img.shape, label)\nimg","metadata":{"id":"tjk-BpXV4idb","outputId":"602b7bdb-02d6-4aba-8c90-092ff4716819","execution":{"iopub.status.busy":"2022-03-13T06:14:02.721054Z","iopub.execute_input":"2022-03-13T06:14:02.721743Z","iopub.status.idle":"2022-03-13T06:14:02.816231Z","shell.execute_reply.started":"2022-03-13T06:14:02.721706Z","shell.execute_reply":"2022-03-13T06:14:02.815553Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes.","metadata":{"id":"poJxIdLb44i-"}},{"cell_type":"code","source":"classes_here=dataset.classes\nprint(classes_here)","metadata":{"id":"0IQXyaQk4o5S","outputId":"615c9741-789a-4578-9c84-db0107b15f9a","execution":{"iopub.status.busy":"2022-03-13T06:14:02.817451Z","iopub.execute_input":"2022-03-13T06:14:02.818123Z","iopub.status.idle":"2022-03-13T06:14:02.822585Z","shell.execute_reply.started":"2022-03-13T06:14:02.818086Z","shell.execute_reply":"2022-03-13T06:14:02.821863Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"We can view the image using `matplotlib`, but we need to change the tensor dimensions to `(142,142,3)`. Let's create a helper function to display an image and its label.","metadata":{"id":"6bFb9htb5B62"}},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'","metadata":{"id":"IMsdxSGi41zK","execution":{"iopub.status.busy":"2022-03-13T06:14:02.826677Z","iopub.execute_input":"2022-03-13T06:14:02.827245Z","iopub.status.idle":"2022-03-13T06:14:02.833972Z","shell.execute_reply.started":"2022-03-13T06:14:02.827104Z","shell.execute_reply":"2022-03-13T06:14:02.833305Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","metadata":{"id":"VV07Gv6g5G5K","execution":{"iopub.status.busy":"2022-03-13T06:14:02.836350Z","iopub.execute_input":"2022-03-13T06:14:02.837206Z","iopub.status.idle":"2022-03-13T06:14:02.842160Z","shell.execute_reply.started":"2022-03-13T06:14:02.837177Z","shell.execute_reply":"2022-03-13T06:14:02.841523Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Let's look at a couple of images from the dataset. As we can tell, the 142x142 px images are quite difficult to identify as fake or real, even for the human eye. Try changing the indices below to view different images.","metadata":{"id":"3UvNLRqz5OEF"}},{"cell_type":"code","source":"# used *arg here for -> img,label=dataset[0] -> shortcut\nshow_example(*dataset[0])","metadata":{"id":"_wy_84mu5ItB","outputId":"6876aa6a-b6ee-4b47-879f-e6a98efabc16","execution":{"iopub.status.busy":"2022-03-13T06:14:02.843380Z","iopub.execute_input":"2022-03-13T06:14:02.844113Z","iopub.status.idle":"2022-03-13T06:14:03.067551Z","shell.execute_reply.started":"2022-03-13T06:14:02.844078Z","shell.execute_reply":"2022-03-13T06:14:03.066862Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"show_example(*dataset[1099])","metadata":{"id":"VTHn6koB5ZV9","outputId":"d11465c5-8cfc-44d3-8f51-128704c591b1","execution":{"iopub.status.busy":"2022-03-13T06:14:03.068669Z","iopub.execute_input":"2022-03-13T06:14:03.069037Z","iopub.status.idle":"2022-03-13T06:14:03.326294Z","shell.execute_reply.started":"2022-03-13T06:14:03.069000Z","shell.execute_reply":"2022-03-13T06:14:03.325476Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"show_example(*dataset[17999])","metadata":{"id":"Yd_Ybqb55kHI","outputId":"02198081-7ecc-41a5-88e5-5b7f9e111acc","execution":{"iopub.status.busy":"2022-03-13T06:14:03.327518Z","iopub.execute_input":"2022-03-13T06:14:03.328148Z","iopub.status.idle":"2022-03-13T06:14:03.570497Z","shell.execute_reply.started":"2022-03-13T06:14:03.328107Z","shell.execute_reply":"2022-03-13T06:14:03.568982Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Training and Validation Datasets\n\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n\nSince there's no predefined validation set, we can set aside a small portion (18000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","metadata":{"id":"3t2rqARu9X-F"}},{"cell_type":"code","source":"random_seed = 43\ntorch.manual_seed(random_seed);","metadata":{"id":"qthOD0Xd58Jr","execution":{"iopub.status.busy":"2022-03-13T06:14:03.573359Z","iopub.execute_input":"2022-03-13T06:14:03.576302Z","iopub.status.idle":"2022-03-13T06:14:03.583549Z","shell.execute_reply.started":"2022-03-13T06:14:03.576261Z","shell.execute_reply":"2022-03-13T06:14:03.582704Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"val_size = 3000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","metadata":{"id":"VtH_Yjkt9dR4","outputId":"2aaf981b-f82e-49bc-cc16-2c4317a8632e","execution":{"iopub.status.busy":"2022-03-13T06:14:03.586048Z","iopub.execute_input":"2022-03-13T06:14:03.589644Z","iopub.status.idle":"2022-03-13T06:14:03.603638Z","shell.execute_reply.started":"2022-03-13T06:14:03.589607Z","shell.execute_reply":"2022-03-13T06:14:03.603046Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"We can now create data loaders for training and validation, to load the data in batches","metadata":{"id":"wIfnbFiq9mSL"}},{"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size=64","metadata":{"id":"Rr0XaERJ9hAv","execution":{"iopub.status.busy":"2022-03-13T06:14:03.607069Z","iopub.execute_input":"2022-03-13T06:14:03.608907Z","iopub.status.idle":"2022-03-13T06:14:03.613812Z","shell.execute_reply.started":"2022-03-13T06:14:03.608871Z","shell.execute_reply":"2022-03-13T06:14:03.613147Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","metadata":{"id":"SC4VdAR19plP","outputId":"2f8911f5-fdb1-42c1-f187-382b7a70adab","execution":{"iopub.status.busy":"2022-03-13T06:14:03.615837Z","iopub.execute_input":"2022-03-13T06:14:03.616689Z","iopub.status.idle":"2022-03-13T06:14:03.629080Z","shell.execute_reply.started":"2022-03-13T06:14:03.616652Z","shell.execute_reply":"2022-03-13T06:14:03.628371Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"We can look at batches of images from the dataset using the `make_grid` method from `torchvision`. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches.\n\nLet's take a look at some sample images from the training dataloader. To display the images, we'll need to _denormalize_ the pixels values to bring them back into the range `(0,1)`.","metadata":{"id":"R47ey2yJ903T"}},{"cell_type":"code","source":"from torchvision.utils import make_grid\nstats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n\n# to remove normalized pixels\ndef denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        denorm_images = denormalize(images, *stats)\n        ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n        break","metadata":{"id":"cpb9bXwE9r-P","execution":{"iopub.status.busy":"2022-03-13T06:14:03.630559Z","iopub.execute_input":"2022-03-13T06:14:03.630971Z","iopub.status.idle":"2022-03-13T06:14:03.648172Z","shell.execute_reply.started":"2022-03-13T06:14:03.630939Z","shell.execute_reply":"2022-03-13T06:14:03.647442Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl)","metadata":{"id":"VGzABSOA93Ou","outputId":"77057a86-6854-47e0-a07a-058e2d917bf2","execution":{"iopub.status.busy":"2022-03-13T06:14:03.652502Z","iopub.execute_input":"2022-03-13T06:14:03.653513Z","iopub.status.idle":"2022-03-13T06:14:11.146264Z","shell.execute_reply.started":"2022-03-13T06:14:03.653475Z","shell.execute_reply":"2022-03-13T06:14:11.144752Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required.","metadata":{"id":"-ptJ2Xf2zWMZ"}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"id":"7c2N0VWQYtty","execution":{"iopub.status.busy":"2022-03-13T06:14:11.147497Z","iopub.execute_input":"2022-03-13T06:14:11.147738Z","iopub.status.idle":"2022-03-13T06:14:11.157882Z","shell.execute_reply.started":"2022-03-13T06:14:11.147707Z","shell.execute_reply":"2022-03-13T06:14:11.157238Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"id":"NsYP2J7VzWMZ","outputId":"29d40ff2-d9a3-46e3-c239-e0e06a6e5b78","execution":{"iopub.status.busy":"2022-03-13T06:14:11.159468Z","iopub.execute_input":"2022-03-13T06:14:11.159986Z","iopub.status.idle":"2022-03-13T06:14:11.168714Z","shell.execute_reply.started":"2022-03-13T06:14:11.159953Z","shell.execute_reply":"2022-03-13T06:14:11.167981Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","metadata":{"id":"rV6mYYd0zWMZ"}},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","metadata":{"id":"28Ekn5p_zWMZ","execution":{"iopub.status.busy":"2022-03-13T06:14:11.170057Z","iopub.execute_input":"2022-03-13T06:14:11.170576Z","iopub.status.idle":"2022-03-13T06:14:11.175368Z","shell.execute_reply.started":"2022-03-13T06:14:11.170491Z","shell.execute_reply":"2022-03-13T06:14:11.174666Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F","metadata":{"id":"I4AQvJYOYViw","execution":{"iopub.status.busy":"2022-03-13T06:14:11.176926Z","iopub.execute_input":"2022-03-13T06:14:11.177480Z","iopub.status.idle":"2022-03-13T06:14:11.183933Z","shell.execute_reply.started":"2022-03-13T06:14:11.177446Z","shell.execute_reply":"2022-03-13T06:14:11.183200Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","metadata":{"id":"Fs488cE_94_e","execution":{"iopub.status.busy":"2022-03-13T06:14:11.185473Z","iopub.execute_input":"2022-03-13T06:14:11.186065Z","iopub.status.idle":"2022-03-13T06:14:11.199672Z","shell.execute_reply.started":"2022-03-13T06:14:11.186028Z","shell.execute_reply":"2022-03-13T06:14:11.198762Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Model with Residual Blocks and Batch Normalization\n\nOne of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n\n![](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n\nSo, lets build our model now:","metadata":{"id":"mqVO1zqUX-te"}},{"cell_type":"code","source":"# def conv_block(in_channels, out_channels, pool=False):\n#     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n#               nn.BatchNorm2d(out_channels), \n#               nn.ReLU(inplace=True)]\n#     if pool: layers.append(nn.MaxPool2d(2))\n#     return nn.Sequential(*layers)\n\n# class ResNet9(ImageClassificationBase):\n#     def __init__(self, in_channels, num_classes):\n#         super().__init__()\n        \n#         self.conv1 = conv_block(in_channels, 64) # output: 64 x 128 x 128\n#         self.conv2 = conv_block(64, 128, pool=True) # output: 128 x 64 x 64 ; as pool is true\n#         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n#         self.conv3 = conv_block(128, 256, pool=True)  # output: 256 x 32 x 32 \n#         self.conv4 = conv_block(256, 512, pool=True)  # output: 512 x 16 x 16 \n#         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n#         self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1), \n#                                         nn.Flatten(), \n#                                         nn.Dropout(0.2),\n#                                         nn.Linear(512, num_classes)) # output: 512 x 1 x 1 \n        \n#     def forward(self, xb):\n#         out = self.conv1(xb)\n#         out = self.conv2(out)\n#         out = self.res1(out) + out\n#         out = self.conv3(out)\n#         out = self.conv4(out)\n#         out = self.res2(out) + out\n#         out = self.classifier(out)\n#         return out","metadata":{"id":"-PYBa9PoqbFV","execution":{"iopub.status.busy":"2022-03-13T06:14:11.201165Z","iopub.execute_input":"2022-03-13T06:14:11.201796Z","iopub.status.idle":"2022-03-13T06:14:11.210547Z","shell.execute_reply.started":"2022-03-13T06:14:11.201762Z","shell.execute_reply":"2022-03-13T06:14:11.209788Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 34) # output: 34 x 142 x 142\n        self.conv2 = conv_block(34, 64, pool=True) # output: 64 x 71 x 71 ; as pool is true\n        self.res1 = nn.Sequential(conv_block(64, 64), conv_block(64, 64))\n        \n        self.conv3 = conv_block(64, 128) # output: 128 x 71 x 71\n        self.conv4 = conv_block(128,256, pool=True) # output: 256 x 35 x 35\n        self.res2 = nn.Sequential(conv_block(256,256), conv_block(256,256))\n        \n        self.conv5 = conv_block(256,512)  # output: 512 x 35 x 35\n        self.conv6 = conv_block(512, 512, pool=True)  # output: 512 x 17 x 17 \n        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1),  # output: 512 x 17/(17) x 17/(17)\n                                        nn.Flatten(), \n                                        nn.Dropout(0.2),\n                                        nn.Linear(512, num_classes)) # output: 512 x 1 x 1 \n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.conv5(out)\n        out = self.conv6(out)\n        out = self.res3(out) + out\n        out = self.classifier(out)\n        return out","metadata":{"id":"UYmHzJnlExc_","execution":{"iopub.status.busy":"2022-03-13T06:14:11.212017Z","iopub.execute_input":"2022-03-13T06:14:11.212320Z","iopub.status.idle":"2022-03-13T06:14:11.227267Z","shell.execute_reply.started":"2022-03-13T06:14:11.212286Z","shell.execute_reply":"2022-03-13T06:14:11.226426Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model = to_device(ResNet9(3, 2), device)\nmodel","metadata":{"id":"AwA7a2xVYmaO","outputId":"786c7f5b-c36b-473b-f754-dde97041ded7","execution":{"iopub.status.busy":"2022-03-13T06:14:11.228832Z","iopub.execute_input":"2022-03-13T06:14:11.229143Z","iopub.status.idle":"2022-03-13T06:14:11.339367Z","shell.execute_reply.started":"2022-03-13T06:14:11.229096Z","shell.execute_reply":"2022-03-13T06:14:11.338610Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nfor images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","metadata":{"id":"kqxraiWCYqWK","outputId":"cb6584c7-467c-4105-84b4-5b00aed0402a","execution":{"iopub.status.busy":"2022-03-13T06:14:11.340691Z","iopub.execute_input":"2022-03-13T06:14:11.340934Z","iopub.status.idle":"2022-03-13T06:14:19.821535Z","shell.execute_reply.started":"2022-03-13T06:14:11.340902Z","shell.execute_reply":"2022-03-13T06:14:19.820632Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"id":"r8pT4IjtzWMa","execution":{"iopub.status.busy":"2022-03-13T06:14:19.822911Z","iopub.execute_input":"2022-03-13T06:14:19.823221Z","iopub.status.idle":"2022-03-13T06:14:19.835707Z","shell.execute_reply.started":"2022-03-13T06:14:19.823187Z","shell.execute_reply":"2022-03-13T06:14:19.834692Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","metadata":{"id":"pNYtWgQizWMa"}},{"cell_type":"code","source":"history = [evaluate(model, val_dl)]\nhistory","metadata":{"id":"kBc2KvmOzWMa","outputId":"b85a7036-c219-43bb-a68e-a57155b10a60","execution":{"iopub.status.busy":"2022-03-13T06:14:19.837302Z","iopub.execute_input":"2022-03-13T06:14:19.837748Z","iopub.status.idle":"2022-03-13T06:14:46.638200Z","shell.execute_reply.started":"2022-03-13T06:14:19.837706Z","shell.execute_reply":"2022-03-13T06:14:46.637316Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"The initial accuracy is around 50%, which is what one might expect from a randomly intialized model (since it has a 1 in 2 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. ","metadata":{"id":"lxV4IMOJzWMa"}},{"cell_type":"code","source":"epochs = 20\nmax_lr = 0.001 # best1 for lr=0.0001 and size: 64 x 64 and batch size=50 over 99% GPU usage\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","metadata":{"id":"V9k4j_3IzWMa","execution":{"iopub.status.busy":"2022-03-13T06:14:46.639744Z","iopub.execute_input":"2022-03-13T06:14:46.640322Z","iopub.status.idle":"2022-03-13T06:14:46.646088Z","shell.execute_reply.started":"2022-03-13T06:14:46.640278Z","shell.execute_reply":"2022-03-13T06:14:46.645009Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)\n# time= 20min 30s","metadata":{"id":"trmY10h2zWMa","outputId":"7f5a3520-aacf-40ca-9540-e4099f053ea3","execution":{"iopub.status.busy":"2022-03-13T06:14:46.647360Z","iopub.execute_input":"2022-03-13T06:14:46.647705Z","iopub.status.idle":"2022-03-13T07:19:28.668533Z","shell.execute_reply.started":"2022-03-13T06:14:46.647669Z","shell.execute_reply":"2022-03-13T07:19:28.667674Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(12, 0.00001, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","metadata":{"id":"XgT3rbtLExdE","execution":{"iopub.status.busy":"2022-03-13T07:24:04.532466Z","iopub.execute_input":"2022-03-13T07:24:04.533017Z","iopub.status.idle":"2022-03-13T08:02:50.086928Z","shell.execute_reply.started":"2022-03-13T07:24:04.532977Z","shell.execute_reply":"2022-03-13T08:02:50.086070Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"%%time\n# history += fit_one_cycle(25, max_lr, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","metadata":{"id":"yO0YyKiDExdE","execution":{"iopub.status.busy":"2022-03-13T08:02:50.094001Z","iopub.execute_input":"2022-03-13T08:02:50.094784Z","iopub.status.idle":"2022-03-13T08:02:50.099978Z","shell.execute_reply.started":"2022-03-13T08:02:50.094741Z","shell.execute_reply":"2022-03-13T08:02:50.099283Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# %%time\n# history += fit_one_cycle(12, 0.0001, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","metadata":{"id":"GjFqZ1umExdF","execution":{"iopub.status.busy":"2022-03-13T08:02:50.101203Z","iopub.execute_input":"2022-03-13T08:02:50.101684Z","iopub.status.idle":"2022-03-13T08:02:50.110905Z","shell.execute_reply.started":"2022-03-13T08:02:50.101648Z","shell.execute_reply":"2022-03-13T08:02:50.110209Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"After running the model for about 20-30 minutes we extract the best possible accuracy on validation set which is : ~98%.\nNow, lets try to plot how accuracy vs epochs and learning rate vs epoch as shown below:","metadata":{"id":"3rzuvqh1YkRE"}},{"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","metadata":{"id":"ldfZxhIMzWMa","execution":{"iopub.status.busy":"2022-03-13T08:02:50.112879Z","iopub.execute_input":"2022-03-13T08:02:50.113130Z","iopub.status.idle":"2022-03-13T08:02:50.123013Z","shell.execute_reply.started":"2022-03-13T08:02:50.113097Z","shell.execute_reply":"2022-03-13T08:02:50.122319Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(history)","metadata":{"id":"sDpWVpd2zWMa","execution":{"iopub.status.busy":"2022-03-13T08:02:50.124424Z","iopub.execute_input":"2022-03-13T08:02:50.124994Z","iopub.status.idle":"2022-03-13T08:02:50.308944Z","shell.execute_reply.started":"2022-03-13T08:02:50.124942Z","shell.execute_reply":"2022-03-13T08:02:50.308240Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","metadata":{"id":"ZPnfMHALzWMa","execution":{"iopub.status.busy":"2022-03-13T08:02:50.310164Z","iopub.execute_input":"2022-03-13T08:02:50.310392Z","iopub.status.idle":"2022-03-13T08:02:50.316012Z","shell.execute_reply.started":"2022-03-13T08:02:50.310361Z","shell.execute_reply":"2022-03-13T08:02:50.315233Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"plot_losses(history)","metadata":{"id":"hh_wag6UzWMa","execution":{"iopub.status.busy":"2022-03-13T08:02:50.317389Z","iopub.execute_input":"2022-03-13T08:02:50.317856Z","iopub.status.idle":"2022-03-13T08:02:50.533770Z","shell.execute_reply.started":"2022-03-13T08:02:50.317820Z","shell.execute_reply":"2022-03-13T08:02:50.533143Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');\nplot_lrs(history)","metadata":{"id":"Vy9w64H8ExdG","execution":{"iopub.status.busy":"2022-03-13T08:02:50.534912Z","iopub.execute_input":"2022-03-13T08:02:50.535336Z","iopub.status.idle":"2022-03-13T08:02:50.721118Z","shell.execute_reply.started":"2022-03-13T08:02:50.535298Z","shell.execute_reply":"2022-03-13T08:02:50.720468Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"## Testing with individual images\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined validation dataset of 9000 images.\n","metadata":{"id":"0XRWhqLa0hWn"}},{"cell_type":"code","source":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","metadata":{"id":"v8H_ECWhExdH","execution":{"iopub.status.busy":"2022-03-13T08:02:50.722279Z","iopub.execute_input":"2022-03-13T08:02:50.722669Z","iopub.status.idle":"2022-03-13T08:02:50.728183Z","shell.execute_reply.started":"2022-03-13T08:02:50.722634Z","shell.execute_reply":"2022-03-13T08:02:50.727467Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"img, label = val_ds[0]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', classes_here[label], ', Predicted:', predict_image(img, model))","metadata":{"id":"mLmpSgk5ExdH","execution":{"iopub.status.busy":"2022-03-13T08:02:50.730743Z","iopub.execute_input":"2022-03-13T08:02:50.731184Z","iopub.status.idle":"2022-03-13T08:02:50.923523Z","shell.execute_reply.started":"2022-03-13T08:02:50.731053Z","shell.execute_reply":"2022-03-13T08:02:50.922829Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"id":"a5kPrCe_ExdH","execution":{"iopub.status.busy":"2022-03-13T08:02:50.924798Z","iopub.execute_input":"2022-03-13T08:02:50.925212Z","iopub.status.idle":"2022-03-13T08:02:50.938310Z","shell.execute_reply.started":"2022-03-13T08:02:50.925174Z","shell.execute_reply":"2022-03-13T08:02:50.937369Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# ***Preparing Final Submission***\n\nMaking prediction on test set and saving prediction to 'submit_me.csv' file for final submission.","metadata":{"id":"_4NLr79kY_JD"}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'gender_classification_trained_model_final2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:02:50.941100Z","iopub.execute_input":"2022-03-13T08:02:50.941290Z","iopub.status.idle":"2022-03-13T08:02:51.052413Z","shell.execute_reply.started":"2022-03-13T08:02:50.941267Z","shell.execute_reply":"2022-03-13T08:02:51.051574Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from PIL import Image\n\n# # test images directory\n# test_dir=data_dir+'/test'\n# submission=pd.read_csv(data_dir+\"/sample_submission.csv\")\n# final_submission=submission.copy()\n\n# # to trabsform image\n# transforms_test=tt.Compose([tt.Resize(142), \n#                             tt.RandomHorizontalFlip(), \n#                              #tt.RandomResizedCrop(64, scale=(0.5,0.9), ratio=(1, 1)), \n#                              #tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n#                              #tt.RandomCrop(64, padding=4, padding_mode='reflect'),\n#                              tt.ToTensor(),\n#                              tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n#                                             ])\n\n# for img in os.listdir(test_dir):\n#     img_str=img\n#     address=test_dir+\"/\"+str(img)\n#     img=Image.open(address)\n#     img=transforms_test(img)\n#     prediction=predict_image(img, model)\n#     final_submission.loc[final_submission['path']=='data/test/'+str(img_str),'label']=str(prediction)\n    \n#     # to print image uncomment this\n#     #plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n#     #print('Label:', classes_here[label], ', Predicted:', predict_image(img, model))\n    \n    \n# final_submission.to_csv('Submit_me.csv', index=False)\n\n\n\n\n\n\n#test_dataset = ImageFolder(data_dir+'/test', )","metadata":{"id":"xZOfL9OEzWMa","execution":{"iopub.status.busy":"2022-03-13T08:02:51.053750Z","iopub.execute_input":"2022-03-13T08:02:51.054255Z","iopub.status.idle":"2022-03-13T08:02:51.059861Z","shell.execute_reply.started":"2022-03-13T08:02:51.054215Z","shell.execute_reply":"2022-03-13T08:02:51.059144Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}